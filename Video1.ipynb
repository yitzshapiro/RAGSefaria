{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i>SinAI</i> (The RAG Sefaria Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before running this notebook, be sure you've started Docker and are running a Qdrant instance. For more info, please visit [Qdrant's Website](https://qdrant.tech/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OpenAI Api Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=<openai_api_key> # Replace with your OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-index-vector-stores-qdrant llama-index-readers-file llama-index-llms-openai llama-index-embeddings-openai qdrant_client redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import qdrant_client\n",
    "from qdrant_client import models\n",
    "from IPython.display import display, Markdown\n",
    "from llama_index.core import VectorStoreIndex, Settings, Document\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-3-large', embed_batch_size=100) #Replace with preferred embedding model\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Qdrant client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"torah\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(base_dir):\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dirname in dirs:\n",
    "            hebrew_dir = os.path.join(root, dirname, \"Hebrew\")\n",
    "            json_file = os.path.join(hebrew_dir, \"merged.json\")\n",
    "            if os.path.exists(json_file):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    book = data.get('title', '')\n",
    "                    language = data.get('language', '')\n",
    "                    for chapter_index, chapter in enumerate(data.get('text', [])):\n",
    "                        for verse_index, verse in enumerate(chapter):\n",
    "                            doc = Document(\n",
    "                                text=verse,\n",
    "                                metadata={\n",
    "                                    \"book\": book,\n",
    "                                    \"language\": language,\n",
    "                                    \"chapter\": chapter_index + 1,\n",
    "                                    \"verse\": verse_index + 1\n",
    "                                }\n",
    "                            )\n",
    "                            documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torah_documents = process_files('/path/to/data/json/Tanakh/Torah') # Replace with your Torah directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torah_documents[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Torah Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torah_vector_store = QdrantVectorStore(client=client, collection_name=\"torah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=torah_vector_store,\n",
    "    documents=torah_documents\n",
    ")\n",
    "\n",
    "pipeline.run(torah_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torah_index = VectorStoreIndex.from_vector_store(torah_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = torah_index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "response = query_engine.query(\"Which pasukim tell me about the laws of Pesach?\")\n",
    "\n",
    "display(Markdown(f\"<b>{response.response}</b>\"))\n",
    "for pasuk in response.source_nodes:\n",
    "    pprint.pp(pasuk.text)\n",
    "    pprint.pp(pasuk.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nevi'im Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"neviim\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "neviim_docs = process_files('/path/to/data/json/Tanakh/Prophets') # Replace with your Neviim directory\n",
    "\n",
    "neviim_vector_store = QdrantVectorStore(client=client, collection_name=\"neviim\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=neviim_vector_store,\n",
    "    documents=neviim_docs,\n",
    ")\n",
    "\n",
    "pipeline.run(neviim_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neviim_index = VectorStoreIndex.from_vector_store(neviim_vector_store)\n",
    "neviim_query_engine = neviim_index.as_query_engine(similarity_top_k=20)\n",
    "response = neviim_query_engine.query(\"Can you detail the story of Isaiah with relevant verses?\")\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ketuvim Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"ketuvim\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "ketuvim_docs = process_files('/path/to/your/data/json/Tanakh/Writings') # Replace with your Ketuvim directory\n",
    "\n",
    "ketuvim_vector_store = QdrantVectorStore(client=client, collection_name=\"ketuvim\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=ketuvim_vector_store,\n",
    "    documents=ketuvim_docs,\n",
    ")\n",
    "\n",
    "pipeline.run(ketuvim_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Mishnah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mishna(base_dir):\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        sederpath = os.path.normpath(root).split(os.sep)\n",
    "        seder = sederpath[-1]\n",
    "        seder = seder.replace(\"Seder \", \"\") \n",
    "        for dirname in dirs:\n",
    "            hebrew_dir = os.path.join(root, dirname, \"Hebrew\")\n",
    "            json_file = os.path.join(hebrew_dir, \"merged.json\")\n",
    "            if os.path.exists(json_file):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    masechta = data.get('title', '')\n",
    "                    masechta = masechta.replace(\"Mishnah \", \"\")\n",
    "                    language = data.get('language', '')\n",
    "                    for chapter_index, chapter in enumerate(data.get('text', [])):\n",
    "                        for mishna_index, mishna in enumerate(chapter):\n",
    "                            doc = Document(\n",
    "                                text=mishna,\n",
    "                                metadata={\n",
    "                                    \"seder\": seder,\n",
    "                                    \"masechta\": masechta,\n",
    "                                    \"language\": language,\n",
    "                                    \"chapter\": chapter_index + 1,  # chapters are 1-indexed\n",
    "                                    \"mishna\": mishna_index + 1      # verses are 1-indexed\n",
    "                                }\n",
    "                            )\n",
    "                            documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"mishna\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "mishna_docs = process_mishna('/path/to/your/data/json/Mishnah/Mishna') # Replace with your Mishna directory\n",
    "\n",
    "mishna_vector_store = QdrantVectorStore(client=client, collection_name=\"mishna\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=mishna_vector_store,\n",
    "    documents=mishna_docs,\n",
    ")\n",
    "\n",
    "pipeline.run(mishna_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Talmud Bavli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!redis-server --daemonize yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amud_to_daf(amud):\n",
    "    base = (amud - 3) // 2 + 2\n",
    "    is_a = (amud % 2) != 0\n",
    "    return f\"{base}{'a' if is_a else 'b'}\"\n",
    "\n",
    "def process_talmud(base_dir):\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        sederpath = os.path.normpath(root).split(os.sep)\n",
    "        seder = sederpath[-1]\n",
    "        seder = seder.replace(\"Seder \", \"\") \n",
    "        for dirname in dirs:\n",
    "            hebrew_dir = os.path.join(root, dirname, \"Hebrew\")\n",
    "            json_file = os.path.join(hebrew_dir, \"merged.json\")\n",
    "            if os.path.exists(json_file):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    masechta = data.get('title', '')\n",
    "                    language = data.get('language', '')\n",
    "                    for chapter_index, chapter in enumerate(data.get('text', [])):\n",
    "                        if chapter_index < 2:\n",
    "                            continue\n",
    "                        amud = chapter_index\n",
    "                        for sentence_index, sentence in enumerate(chapter):\n",
    "                            doc = Document(\n",
    "                                text=sentence,\n",
    "                                metadata={\n",
    "                                    \"seder\": seder,\n",
    "                                    \"masechta\": masechta,\n",
    "                                    \"language\": language,\n",
    "                                    \"daf\": amud_to_daf(amud + 1),\n",
    "                                    \"sentence\": sentence_index + 1\n",
    "                                }\n",
    "                            )\n",
    "                            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "def batch_documents(documents, batch_size):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        yield documents[i:i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"talmud_bavli\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "talmud_docs = process_talmud('/path/to/your/data/json/Talmud/Bavli/Gemara') # Replace with your Gemara directory\n",
    "talmud_vector_store = QdrantVectorStore(client=client, collection_name=\"talmud\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=talmud_vector_store,\n",
    ")\n",
    "\n",
    "for batch in batch_documents(talmud_docs, 4000):\n",
    "    pipeline.run(documents=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Rishonim on the Talmud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import redis\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "logging.basicConfig(filename='rishonim.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-3-large', embed_batch_size=100)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "def amud_to_daf(amud):\n",
    "    base = (amud - 3) // 2 + 2\n",
    "    is_a = (amud % 2) != 0\n",
    "    return f\"{base}{'a' if is_a else 'b'}\"\n",
    "\n",
    "def generate_document_hash(document):\n",
    "    hash_input = f\"{document.text}{document.metadata['rishon']}{document.metadata['seder']}{document.metadata['masechta']}{document.metadata['language']}{document.metadata['amud']}{document.metadata['sentence']}\"\n",
    "    return hashlib.sha256(hash_input.encode()).hexdigest()\n",
    "\n",
    "def flatten_text(nested_list):\n",
    "    \"\"\" Recursively flatten a list and yield non-empty strings. \"\"\"\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, str) and item.strip():\n",
    "            yield item\n",
    "        elif isinstance(item, list):\n",
    "            yield from flatten_text(item)\n",
    "\n",
    "def process_talmud(base_dir, rishon):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        sederpath = os.path.normpath(root).split(os.sep)\n",
    "        seder = sederpath[-1].replace(\"Seder \", \"\")\n",
    "        for dirname in dirs:\n",
    "            hebrew_dir = os.path.join(root, dirname, \"Hebrew\")\n",
    "            json_file = os.path.join(hebrew_dir, \"merged.json\")\n",
    "            if os.path.exists(json_file):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    masechta = data.get('title', '').replace(f\"{rishon} on \", \"\").replace(f\"{rishon} \", \"\")\n",
    "                    language = data.get('language', '')\n",
    "                    for amud_index, amud in enumerate(data.get('text', [])):\n",
    "                        if amud_index < 2:\n",
    "                            continue\n",
    "                        sentence_index = 0\n",
    "                        for sentence in flatten_text(amud):\n",
    "                            sentence_index += 1\n",
    "                            yield Document(\n",
    "                                text=sentence,\n",
    "                                metadata={\n",
    "                                    \"rishon\": rishon,\n",
    "                                    \"seder\": seder,\n",
    "                                    \"masechta\": masechta,\n",
    "                                    \"language\": language,\n",
    "                                    \"daf\": amud_to_daf(amud_index + 1),\n",
    "                                    \"sentence\": sentence_index,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "def process_all_rishonim(directory):\n",
    "    all_documents = []\n",
    "    for rishon in os.listdir(directory):\n",
    "        rishon_dir = os.path.join(directory, rishon)\n",
    "        if os.path.isdir(rishon_dir):\n",
    "            documents = process_talmud(rishon_dir, rishon)\n",
    "            all_documents.extend(documents)\n",
    "    return all_documents\n",
    "\n",
    "def batch_documents(documents, batch_size):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        yield documents[i:i + batch_size]\n",
    "\n",
    "def ingest_documents(directory, batch_size=4000):\n",
    "    documents = []\n",
    "    redis_pipeline = redis_client.pipeline()\n",
    "\n",
    "    for rishon in os.listdir(directory):\n",
    "        rishon_dir = os.path.join(directory, rishon)\n",
    "        if os.path.isdir(rishon_dir):\n",
    "            for document in process_talmud(rishon_dir, rishon):\n",
    "                doc_hash = generate_document_hash(document)\n",
    "                redis_pipeline.sismember(\"processed_hashes\", doc_hash)\n",
    "                documents.append((document, doc_hash))\n",
    "                \n",
    "                if len(documents) >= batch_size:\n",
    "                    results = redis_pipeline.execute()\n",
    "                    new_documents = []\n",
    "                    hashes_to_add = []\n",
    "                    for (doc, hash), exists in zip(documents, results):\n",
    "                        if not exists:\n",
    "                            new_documents.append(doc)\n",
    "                            hashes_to_add.append(hash)\n",
    "\n",
    "                    if new_documents:\n",
    "                        pipeline.run(documents=new_documents)\n",
    "                        for hash in hashes_to_add:\n",
    "                            redis_client.sadd(\"processed_hashes\", hash)\n",
    "                        logging.info(f\"Ingesting {len(new_documents)} documents\")\n",
    "                        logging.info(f\"{new_documents[0].metadata if new_documents else 'No new documents'}\")\n",
    "                    \n",
    "                    documents = []\n",
    "                    redis_pipeline = redis_client.pipeline()\n",
    "\n",
    "    if documents:\n",
    "        results = redis_pipeline.execute()\n",
    "        new_documents = []\n",
    "        hashes_to_add = []\n",
    "        for (doc, hash), exists in zip(documents, results):\n",
    "            if not exists:\n",
    "                new_documents.append(doc)\n",
    "                hashes_to_add.append(hash)\n",
    "\n",
    "        if new_documents:\n",
    "            pipeline.run(documents=new_documents)\n",
    "            for hash in hashes_to_add:\n",
    "                redis_client.sadd(\"processed_hashes\", hash)\n",
    "\n",
    "            logging.info(f\"Ingesting {len(new_documents)} documents\")\n",
    "            logging.info(f\"{new_documents[0].metadata if new_documents else 'No new documents'}\")\n",
    "\n",
    "    redis_pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"rishonim_bavli\",\n",
    "    vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "rishonim_vector_store = QdrantVectorStore(client=client, collection_name=\"rishonim_bavli\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        splitter,\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=rishonim_vector_store,\n",
    ")\n",
    "\n",
    "ingest_documents('/path/to/json/Talmud/Bavli/Rishonim on Talmud') # Replace with your Rishonim directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!redis-cli shutdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
